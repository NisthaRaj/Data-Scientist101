{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7154c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the Pyspark Environment variables\n",
    "# Set the PySpark environment variables\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = \"/Users/nistharaj/Desktop/Spark\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9874903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c522b7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/14 13:25:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"RDD-Demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a2402",
   "metadata": {},
   "source": [
    "# Advantages of RDDs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db92f974",
   "metadata": {},
   "source": [
    "1. Backbone of Data Processing in Spark.\n",
    "2. Distributed, fault-tolerant and parallelizable data structure.\n",
    "3. Efficiently processes large datasets across a cluster\n",
    "4. Key Characteristics: immutable, distributed, resilient, lazily evaluated, fault tolerant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6f8e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3e68332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect action: Retrieve all elements of the RDD\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99b5c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from a list of tuples\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35), (\"Alice\", 40)]\n",
    "rdd = spark.sparkContext.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c8261",
   "metadata": {},
   "source": [
    "RDD Operations: Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6a991",
   "metadata": {},
   "source": [
    "1. Returns results or perform actions in RDD, triggering execution.\n",
    "2. Eager evaluation data movements/computation\n",
    "3. Examples: Collect,count,first,take,save,foreach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f05ee3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of elements in rdd:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 8) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count action: Count the number of elements in the RDD\n",
    "count = rdd.count()\n",
    "print(\"The total number of elements in rdd: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c030675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first element of the rdd:  ('Alice', 25)\n"
     ]
    }
   ],
   "source": [
    "# First action: Retrieve the first element of the RDD\n",
    "first_element = rdd.first()\n",
    "print(\"The first element of the rdd: \", first_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46d148a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first two elements of the rdd:  [('Alice', 25), ('Bob', 30)]\n"
     ]
    }
   ],
   "source": [
    "# Take action: Retrieve the n elements of the RDD\n",
    "taken_elements = rdd.take(2)\n",
    "print(\"The first two elements of the rdd: \", taken_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56cab627",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('Alice', 25)\n",
      "('Charlie', 35)\n",
      "('Bob', 30)\n",
      "('Alice', 40)\n"
     ]
    }
   ],
   "source": [
    "# Foreach action: Print each element of the RDD\n",
    "rdd.foreach(lambda x:print(x))\n",
    "#rdd.foreach(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6256434",
   "metadata": {},
   "source": [
    "RDD Operations: Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c90d806",
   "metadata": {},
   "source": [
    "1. Create new RDDs by applying computation/manipulation\n",
    "2. Lazy evaluation, lineage graph\n",
    "3. Examples:map,filter,flatMap,reduceByKey,SortBy,Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0da3b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map transformation: Convert name to uppercase\n",
    "mapped_rdd = rdd.map(lambda x: (x[0].upper(), x[1]))\n",
    "#mapped_rdd = rdd.map(lambda x: (x[0].upper(), x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5348ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd with uppercease name:  [('ALICE', 25), ('BOB', 30), ('CHARLIE', 35), ('ALICE', 40)]\n"
     ]
    }
   ],
   "source": [
    "result = mapped_rdd.collect()\n",
    "#result = mapped_rdd.collect()\n",
    "print(\"rdd with uppercease name: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efbe29f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Charlie', 35), ('Alice', 40)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter transformation: Filter records where age is greater than 30\n",
    "filtered_rdd = rdd.filter(lambda x:x[1] > 30)\n",
    "filtered_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74b3fbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Charlie', 35), ('Alice', 65), ('Bob', 30)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ReduceByKey transformation: Calculate the total age for each name\n",
    "reduced_rdd = rdd.reduceByKey(lambda x, y: x+y)\n",
    "reduced_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2371a939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', 40), ('Charlie', 35), ('Bob', 30), ('Alice', 25)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SortBy transformation: Sort the RDD by age in descending order\n",
    "sorted_rdd = rdd.sortBy(lambda x:x[1], ascending = False)\n",
    "#sorted_rdd = rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "sorted_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb398838",
   "metadata": {},
   "source": [
    "Save RDDs to text file and read RDDs from text file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7d086c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save action: Save the RDD to a text file\n",
    "rdd.saveAsTextFile(\"output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "225d04c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"('Charlie', 35)\", \"('Bob', 30)\", \"('Alice', 25)\", \"('Alice', 40)\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create rdd from text file\n",
    "rdd_text = spark.sparkContext.textFile(\"output.txt\")\n",
    "rdd_text.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233d7133",
   "metadata": {},
   "source": [
    "Shut down Spark Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3893a6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d36cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
